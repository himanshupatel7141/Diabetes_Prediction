# -*- coding: utf-8 -*-
"""Diabetes_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yd0W2lvEk1DOfIeRuCMFshh2D9kGVYlb
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
###importing the data set
dataframe= pd.read_csv("/content/diabetes.csv")
print(f'\n\n**dataframehead**\n\n:{dataframe.head()}')
##checking the colums name
print(f'\n\n**colums**\n\n:{dataframe.columns}')
##checking number of null values
print(f'\n\n**nullvalues**\n\n:{dataframe.isnull().sum()}')
##data types in each column
dataframe.dtypes

"""**EDA**"""

dataframe.corr()
plt.figure(figsize=(15,15))
print(sns.heatmap(dataframe.corr(),annot=True))
print(dataframe.describe())

plt.figure(figsize=(10, 8))

plt.subplot(3, 3, 1)
sns.histplot(dataframe["Pregnancies"], kde=False)
plt.title("Pregnancies")

plt.subplot(3, 3, 2)
sns.histplot(dataframe["Glucose"], kde=False)
plt.title("Glucose")

plt.subplot(3, 3, 3)
sns.histplot(dataframe["BloodPressure"], kde=False)
plt.title("BloodPressure")

plt.subplot(3, 3, 4)
sns.histplot(dataframe["SkinThickness"], kde=False)
plt.title("SkinThickness")

plt.subplot(3, 3, 5)
sns.histplot(dataframe["Insulin"], kde=False)
plt.title("Insulin")

plt.subplot(3, 3, 6)
sns.histplot(dataframe["BMI"], kde=False)
plt.title("BMI")

plt.subplot(3, 3, 7)
sns.histplot(dataframe["DiabetesPedigreeFunction"], kde=False)
plt.title("DiabetesPedigreeFunction")

plt.subplot(3, 3, 8)
sns.histplot(dataframe["Age"], kde=False)
plt.title("Age")


plt.subplot(3, 3, 9)
sns.histplot(dataframe["Outcome"], kde=False)
plt.title("Outcome")

plt.tight_layout()
plt.show()

dataframe['Pregnacies']=dataframe['Pregnancies'].replace(0,dataframe["Pregnancies"].median())
dataframe['Insulin'] = dataframe['Insulin'].replace(0, dataframe['Insulin'].median())
dataframe['BloodPressure']=dataframe['BloodPressure'].replace(0,dataframe["BloodPressure"].median())
dataframe['SkinThickness'] = dataframe['SkinThickness'].replace(0, dataframe['SkinThickness'].median())
dataframe['Insulin']=dataframe['Insulin'].replace(0,dataframe["Insulin"].median())
dataframe['BMI'] = dataframe['BMI'].replace(0, dataframe['BMI'].mean())
dataframe['DiabetesPedigreeFunction']=dataframe['DiabetesPedigreeFunction'].replace(0,dataframe["DiabetesPedigreeFunction"].mean())
dataframe['Age'] = dataframe['Age'].replace(0, dataframe['Age'].median())

"""**Outlier Detection and Normaliztion**"""

print(f'shape:{dataframe.shape}')
print(f'columnname:{dataframe.columns}')
#dropping the target variable from the data set
X = dataframe.drop(columns='Outcome', axis=1)
y = dataframe['Outcome']
fig, ax = plt.subplots(figsize = (15, 15))
print(sns.boxplot(data = X, ax=ax))
plt.savefig('boxPlot.jpg')
## removing the outlier
cols = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']

for col in cols:
    Q1 = X[col].quantile(0.25)
    Q3 = X[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    mask = (X[col] >= lower_bound) & (X[col] <= upper_bound)
# Filter dataset to remove outliers
X_outlier_detection = X[mask]
y_outlier_detection = y[mask]
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_outlier_detection)
fig, ax = plt.subplots(figsize = (15, 15))
print(sns.boxplot(data = X_scaled, ax=ax))
print(type(X_scaled))
print(len(X_scaled))
print(X_scaled.shape)
X_scaled = pd.DataFrame(X_scaled[:, :8], columns=cols)
X_scaled=pd.DataFrame(X_scaled,columns=cols)
print(X_scaled.describe())
print(y_outlier_detection.value_counts())
X_scaled.reset_index(drop=True, inplace=True)
y_outlier_detection.reset_index(drop=True, inplace=True)
cols = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']
for col in cols:
    q = X_scaled[col].quantile(.95)
    mask = X_scaled[col] < q
dataNew = X_scaled[mask]
y_outlier_detection = y_outlier_detection[mask]
ig, ax = plt.subplots(figsize = (15, 15))
print(sns.boxplot(data = dataNew, ax=ax))
plt.savefig('boxPlot.jpg')

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(dataNew, y_outlier_detection, test_size=0.33, random_state=42)
X_train.shape,X_test.shape
print(f'Before applying SMOTE:{y_train.value_counts()}')
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Check resampled class distribution
print("\nResampled class distribution:")
print(f'after applying SMOTE:{pd.Series(y_train_resampled).value_counts()}')

"""**LgigistiocRegression**"""

from sklearn.linear_model import LogisticRegression
# Initialize Logistic Regression
classification = LogisticRegression()
# Fit the model to the training data
classification.fit(X_train_resampled, y_train_resampled)
edictions = classification.predict(X_test)
print(y_predictions)
from sklearn.metrics import accuracy_score
accuracy_score(y_test, y_predictions)

from sklearn.metrics import classification_report
target_names = ['Non-Diabetic', 'Diabetic']
print(classification_report(y_test, y_predictions, target_names=target_names))

from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier()

model.fit(X_train_resampled, y_train_resampled)

y_pred = model.predict(X_train_resampled)
y_pred

from sklearn import tree
plt.figure(figsize=(10, 10))
print(tree.plot_tree(model, filled=True))

""" **RandomForestClassifier**"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()
rf.fit(X_train_resampled, y_train_resampled)
print(f'rf score before optimization:{rf.score(X_test, y_test)}')
y_predict_rf = rf.predict(X_test)
print(y_predict_rf)
grid_param =  {
    "n_estimators" : [50, 100, 120],
    'criterion' : ['gini', 'entropy'],
    'max_depth' : range(5)
}
from sklearn.model_selection import GridSearchCV
grid_search_rf = GridSearchCV(param_grid=grid_param, cv=10, n_jobs=6, verbose=1, estimator=rf)
grid_search_rf.fit(X_train_resampled,y_train_resampled)
print(grid_search_rf.best_params_)
rf_new = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=4)
rf_new.fit(X_train_resampled, y_train_resampled)
print(f'rf score after optimization:{rf.score(X_test, y_test)}')

"""**Support Vector Machines (SVM)**"""

from sklearn.svm import SVC
svm_classifier = SVC()
svm_classifier.fit(X_train_resampled, y_train_resampled)
predictions = svm_classifier.predict(X_test)
from sklearn.model_selection import GridSearchCV
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': ['scale', 'auto'],
    'degree': [2, 3, 4]
}
grid_search = GridSearchCV(svm_classifier, param_grid, cv=5)
grid_search.fit(X_train_resampled, y_train_resampled)
best_params = grid_search.best_params_
print(best_params)
svm_classifier_optimizedparams = SVC(C=10, gamma='auto', kernel='rbf', degree=2)
svm_classifier_optimizedparams.fit(X_train_resampled,y_train_resampled)
score = svm_classifier_optimizedparams.score(X_test, y_test)
print(f'Optimized SVM Test Accuracy: {score}')

""" **K-Nearest Neighbors (KNN)**"""

from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.metrics import classification_report, confusion_matrix
# Instantiating the KNN Classifier
knn = KNeighborsClassifier()

# Fitting the KNN Classifier to the training data
knn.fit(X_train_resampled, y_train_resampled)
y_prediction_knn=knn.predict(X_test)
print(y_prediction_knn)
print("Confusion Matrix")
print(confusion_matrix(y_test, y_prediction_knn))
print("Classification Report")
print(classification_report(y_test,y_prediction_knn))
print(f'knn score without optimization: {knn.score(X_test, y_test)}')

import pickle

import pickle
pickle.dump(classification, open("classification_model.pkl", "wb"))

classification_model = pickle.load(open("classification_model.pkl", "rb"))

classification_model.predict(X_test)

